{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "class LinearModel(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    def forward(self, x): \n",
    "        y_pred = self.linear(x) \n",
    "        return y_pred\n",
    "    \n",
    "model = LinearModel()\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data) \n",
    "    print(epoch, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    "x_test = torch.Tensor([[4.0]]) \n",
    "y_test = model(x_test) \n",
    "print('y_pred = ', y_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5f256",
   "metadata": {},
   "source": [
    "创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 2x3 的全 0 张量\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "\n",
    "# 创建一个 2x3 的全 1 张量\n",
    "b = torch.ones(2, 3)\n",
    "print(b)\n",
    "\n",
    "# 创建一个 2x3 的随机数张量\n",
    "c = torch.randn(2, 3)\n",
    "print(c)\n",
    "\n",
    "# 从 NumPy 数组创建张量\n",
    "import numpy as np\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "# 在指定设备（CPU/GPU）上创建张量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d = torch.randn(2, 3, device=device)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21094b",
   "metadata": {},
   "source": [
    "构造一个简单的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的全连接神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6875b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义输入层大小、隐藏层大小、输出层大小和批量大小\n",
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10\n",
    "\n",
    "# 创建虚拟输入数据和目标数据\n",
    "x = torch.randn(batch_size, n_in)  # 随机生成输入数据\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], \n",
    "                  [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])  # 目标输出数据\n",
    "\n",
    "# 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_in, n_h),  # 输入层到隐藏层的线性变换\n",
    "    nn.ReLU(),            # 隐藏层的ReLU激活函数\n",
    "    nn.Linear(n_h, n_out),  # 隐藏层到输出层的线性变换\n",
    "    nn.Sigmoid()           # 输出层的Sigmoid激活函数\n",
    ")\n",
    "\n",
    "# 定义均方误差损失函数和随机梯度下降优化器\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 学习率为0.01\n",
    "\n",
    "# 用于存储每轮的损失值\n",
    "losses = []\n",
    "\n",
    "# 执行梯度下降算法进行模型训练\n",
    "for epoch in range(50):  # 迭代50次\n",
    "    y_pred = model(x)  # 前向传播，计算预测值\n",
    "    loss = criterion(y_pred, y)  # 计算损失\n",
    "    losses.append(loss.item())  # 记录损失值\n",
    "    print(f'Epoch [{epoch+1}/50], Loss: {loss.item():.4f}')  # 打印损失值\n",
    "\n",
    "    optimizer.zero_grad()  # 清零梯度\n",
    "    loss.backward()  # 反向传播，计算梯度\n",
    "    optimizer.step()  # 更新模型参数\n",
    "\n",
    "# 可视化损失变化曲线\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 51), losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 可视化预测结果与实际目标值对比\n",
    "y_pred_final = model(x).detach().numpy()  # 最终预测值\n",
    "y_actual = y.numpy()  # 实际值\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, batch_size + 1), y_actual, 'o-', label='Actual', color='blue')\n",
    "plt.plot(range(1, batch_size + 1), y_pred_final, 'x--', label='Predicted', color='red')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1300cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成一些随机数据\n",
    "n_samples = 100\n",
    "data = torch.randn(n_samples, 2)  # 生成 100 个二维数据点\n",
    "labels = (data[:, 0]**2 + data[:, 1]**2 < 1).float().unsqueeze(1)  # 点在圆内为1，圆外为0\n",
    "\n",
    "# 可视化数据\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm')\n",
    "plt.title(\"Generated Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成一些随机数据\n",
    "n_samples = 100\n",
    "data = torch.randn(n_samples, 2)  # 生成 100 个二维数据点\n",
    "labels = (data[:, 0]**2 + data[:, 1]**2 < 1).float().unsqueeze(1)  # 点在圆内为1，圆外为0\n",
    "\n",
    "# 可视化数据\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm')\n",
    "plt.title(\"Generated Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "# 定义前馈神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 定义神经网络的层\n",
    "        self.fc1 = nn.Linear(2, 4)  # 输入层有 2 个特征，隐藏层有 4 个神经元\n",
    "        self.fc2 = nn.Linear(4, 1)  # 隐藏层输出到 1 个神经元（用于二分类）\n",
    "        self.sigmoid = nn.Sigmoid()  # 二分类激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # 使用 ReLU 激活函数\n",
    "        x = self.sigmoid(self.fc2(x))  # 输出层使用 Sigmoid 激活函数\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "model = SimpleNN()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # 使用随机梯度下降优化器\n",
    "\n",
    "# 训练\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 每 10 轮打印一次损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 可视化决策边界\n",
    "def plot_decision_boundary(model, data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.1), torch.arange(y_min, y_max, 0.1), indexing='ij')\n",
    "    grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\n",
    "    predictions = model(grid).detach().numpy().reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, predictions, levels=[0, 0.5, 1], cmap='coolwarm', alpha=0.7)\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm', edgecolors='k')\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213f927",
   "metadata": {},
   "source": [
    "卷积神经网络CNN代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4706d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torch.utils.data as Data\n",
    "\n",
    "torch.manual_seed(1)  # 设置随机种子, 用于复现\n",
    "\n",
    "# 超参数\n",
    "EPOCH = 1       # 前向后向传播迭代次数\n",
    "LR = 0.001      # 学习率 learning rate \n",
    "BATCH_SIZE = 50 # 批量训练时候一次送入数据的size\n",
    "DOWNLOAD_MNIST = True \n",
    "\n",
    "# 设备设置（自动使用 GPU 如果可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 下载mnist手写数据集\n",
    "# 训练集\n",
    "train_data = torchvision.datasets.MNIST(  \n",
    "    root = './MNIST/',                      \n",
    "    train = True,                            \n",
    "    transform = torchvision.transforms.ToTensor(),                                                \n",
    "    download=DOWNLOAD_MNIST \n",
    " )\n",
    " \n",
    "# 测试集\n",
    "test_data = torchvision.datasets.MNIST(root='./MNIST/', train=False)  # train设置为False表示获取测试集\n",
    " \n",
    "# 一个批训练 50个样本, 1 channel通道, 图片尺寸 28x28 size:(50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ") \n",
    "#  测试数据预处理；只测试前2000个\n",
    "test_x = torch.unsqueeze(test_data.data,dim=1).float()[:2000] / 255.0\n",
    "# shape from (2000, 28, 28) to (2000, 1, 28, 28)\n",
    "test_y = test_data.targets[:2000]\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(                      # 输入的图片 （1，28，28）\n",
    "                in_channels=1,\n",
    "                out_channels=16,            # 经过一个卷积层之后 （16,28,28）\n",
    "                kernel_size=5,\n",
    "                stride=1,                    # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)      # 经过池化层处理，维度为（16,14,14）\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(                         # 输入（16,14,14）\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),                                 # 输出（32,14,14）\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)        # 输出（32,7,7）\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(32*7*7,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)                     #（batch_size,16,14,14）\n",
    "        x = self.conv2(x)                     # 输出（batch_size,32,7,7）\n",
    "        x = x.view(x.size(0),-1)              # (batch_size,32*7*7)\n",
    "        out = self.out(x)                     # (batch_size,10)\n",
    "        return out\n",
    "\n",
    "cnn = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(),lr=LR) # 定义优化器\n",
    "loss_func = nn.CrossEntropyLoss() # 定义损失函数\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step,(batch_x,batch_y) in enumerate(train_loader):\n",
    "        # 将 batch 数据移动到选定设备\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        pred_y = cnn(batch_x)\n",
    "        loss = loss_func(pred_y,batch_y)\n",
    "\n",
    "        optimizer.zero_grad() # 清空上一层梯度\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            # 评估时关闭梯度以节省内存\n",
    "            with torch.no_grad():\n",
    "                test_x_device = test_x.to(device)\n",
    "                test_output = cnn(test_x_device)\n",
    "                pred_y = torch.max(test_output, 1)[1].cpu().numpy()  # 确保在 CPU 上再转 numpy\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.item())\n",
    "\n",
    "# 打印前十个测试结果和真实结果进行对比\n",
    "with torch.no_grad():\n",
    "    test_output = cnn(test_x[:10].to(device))\n",
    "    pred_y = torch.max(test_output, 1)[1].cpu().numpy()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
